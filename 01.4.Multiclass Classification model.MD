# Clusrtring

* Clustering is a form of unsupervised ML in which observations are grouped into clusters based on similarities in their data values, or features. 
* It doesn't make use of previously known label values to train a model. * In a clustering model, the label is the cluster to which the observation is assigned, based only on its features.

* Example 
    1. suppose a botanist observes a sample of flowers and records the number of leaves and petals on each flower:
        ![Alt text](image-10.png)
    1. There are no known labels in the dataset, just two features.
    1. The goal is not to identify the different types (species) of flower; just to group similar flowers together based on the number of leaves and petals.
        Leaves (x1)	Petals (x2)
        0	5
        0	6
        1	3
        1	3
        1	6
        1	8
        2	3
        2	7
        2	8
Training a clustering model
There are multiple algorithms you can use for clustering. One of the most commonly used algorithms is K-Means clustering, which consists of the following steps:

The feature (x) values are vectorized to define n-dimensional coordinates (where n is the number of features). In the flower example, we have two features: number of leaves (x1) and number of petals (x2). So, the feature vector has two coordinates that we can use to conceptually plot the data points in two-dimensional space ([x1,x2])
You decide how many clusters you want to use to group the flowers - call this value k. For example, to create three clusters, you would use a k value of 3. Then k points are plotted at random coordinates. These points become the center points for each cluster, so they're called centroids.
Each data point (in this case a flower) is assigned to its nearest centroid.
Each centroid is moved to the center of the data points assigned to it based on the mean distance between the points.
After the centroid is moved, the data points may now be closer to a different centroid, so the data points are reassigned to clusters based on the new closest centroid.
The centroid movement and cluster reallocation steps are repeated until the clusters become stable or a predetermined maximum number of iterations is reached.
The following animation shows this process:

Diagram of an animation showing the k-means clustering process.

Evaluating a clustering model
Since there's no known label with which to compare the predicted cluster assignments, evaluation of a clustering model is based on how well the resulting clusters are separated from one another.

There are multiple metrics that you can use to evaluate cluster separation, including:

Average distance to cluster center: How close, on average, each point in the cluster is to the centroid of the cluster.
Average distance to other center: How close, on average, each point in the cluster is to the centroid of all other clusters.
Maximum distance to cluster center: The furthest distance between a point in the cluster and its centroid.
Silhouette: A value between -1 and 1 that summarizes the ratio of distance between points in the same cluster and points in different clusters (The closer to 1, the better the cluster separation).Predict to which of multiple possible classes an observation belongs.

* Example - Calculate probability values for multiple class labels, enabling a model to predict the most probable class for a given observation.
    1. some observations of penguins, in which the flipper length (x) of each penguin is recorded. For each observation, the data includes the penguin species (y), which is encoded as follows: 0: Adelie, 1: Gentoo, 2: Chinstrap
        | Flipper length (x) | Species (y) |
        | -- | -- |
        | 167 | 0 |
        | 172 | 0 |
        | 225 | 2 |
        | 197 | 1 |
        | 189 | 1 |
        | 232 | 2 |
        | 158 | 0 |

* Training a multiclass classification model - use an algorithm to fit the training data to a function that calculates a probability value for each possible class. There are two kinds of algorithm you can use to do this:
    1. One-vs-Rest (OvR) algorithms
    2. Multinomial algorithms

* One-vs-Rest (OvR) algorithms
    1. train a binary classification function for each class, each calculating the probability that the observation is an example of the target class. 
    1. Each function calculates the probability of the observation being a specific class compared to any other class. 
    1. For our penguin species classification model, the algorithm would essentially create three binary classification functions:
        1. f0(x) = P(y=0 | x)
        1. f1(x) = P(y=1 | x)
        1. f2(x) = P(y=2 | x)
    1. Each algorithm produces a sigmoid function that calculates a probability value between 0.0 and 1.0. 
    1. A model trained using this kind of algorithm predicts the class for the function that produces the highest probability output.

* Multinomial algorithms
    1. which creates a single function that returns a multi-valued output. 
    1. The output is a vector (an array of values) that contains the probability distribution for all possible classes - with a probability score for each class which when totaled add up to 1.0: f(x) =[P(y=0|x), P(y=1|x), P(y=2|x)]
    1. An example of this kind of function is a softmax function, which could produce an output like the following example: [0.2, 0.3, 0.5]
    1. The elements in the vector represent the probabilities for classes 0, 1, and 2 respectively; so in this case, the class with the highest probability is 2.

* Evaluating a multiclass classification model - calculate aggregate metrics that take all classes into account.
    1. Let's assume that we've validated our multiclass classifier, and obtained the following results:
        | Flipper length (x)	| Actual species (y)	| Predicted species (ŷ) |
        | -- | -- | --|
        | 165	| 0	| 0 |
        | 171	| 0	| 0 |
        | 205	| 2	| 1 |
        | 195	| 1	| 1 |
        | 183	| 1	| 1 |
        | 221	| 2	| 2 |
        | 214	| 2	| 2 |
    1. The confusion matrix for a multiclass classifier is similar to that of a binary classifier, except that it shows the number of predictions for each combination of predicted (ŷ) and actual class labels (y):
        ![Alt text](image-9.png)
    1. From this confusion matrix, we can determine the metrics for each individual class as follows:
        | Class	| TP | TN | FP	| FN | Accuracy	| Recall | Precision | F1-Score |
        | --	| -- | -- | --	| -- | --	| -- | -- | -- |
        | 0	| 2	| 5	| 0	| 0	| 1.0 | 1.0 | 1.0 | 1.0 | 
        | 1	| 2	| 4	| 1	| 0	| 0.86 | 1.0 | 0.67 | 0.8 | 
        | 2	| 2	| 4	| 0	| 1	| 0.86| 0.67 | 1.0	| 0.8 | 
    1. To calculate the overall accuracy, recall, and precision metrics, you use the total of the TP, TN, FP, and FN metrics:
        1. Overall accuracy = (13+6)÷(13+6+1+1) = 0.90
        1. Overall recall = 6÷(6+1) = 0.86
        1. Overall precision = 6÷(6+1) = 0.86
    1. The overall F1-score is calculated using the overall recall and precision metrics: Overall F1-score = (2x0.86x0.86)÷(0.86+0.86) = 0.86